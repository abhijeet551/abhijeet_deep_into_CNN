{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T09:24:18.361826Z","iopub.execute_input":"2021-06-19T09:24:18.362166Z","iopub.status.idle":"2021-06-19T09:24:18.37289Z","shell.execute_reply.started":"2021-06-19T09:24:18.362135Z","shell.execute_reply":"2021-06-19T09:24:18.371805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nfrom sklearn.preprocessing import LabelEncoder\nfilename = \"/kaggle/input/tabular-playground-series-jun-2021/train.csv\"\ntest_filename=\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\"\ntraindf = pd.read_csv(filename,index_col='id')\ntestdf=pd.read_csv(test_filename,index_col='id')\nlencoder = LabelEncoder()\ny = lencoder.fit_transform(traindf['target']).astype('int64')\ntestdf.tail()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:18.374857Z","iopub.execute_input":"2021-06-19T09:24:18.375766Z","iopub.status.idle":"2021-06-19T09:24:19.904464Z","shell.execute_reply.started":"2021-06-19T09:24:18.375723Z","shell.execute_reply":"2021-06-19T09:24:19.903595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y)\nprint(type(y))\ntraindf['target']=y\ntraindf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:19.906373Z","iopub.execute_input":"2021-06-19T09:24:19.906774Z","iopub.status.idle":"2021-06-19T09:24:19.931078Z","shell.execute_reply.started":"2021-06-19T09:24:19.90673Z","shell.execute_reply":"2021-06-19T09:24:19.930116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputSize = len(traindf.columns) -1\nnumClasses = traindf['target'].nunique()\nbatchSize = 1024","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:19.932677Z","iopub.execute_input":"2021-06-19T09:24:19.933017Z","iopub.status.idle":"2021-06-19T09:24:19.938977Z","shell.execute_reply.started":"2021-06-19T09:24:19.932981Z","shell.execute_reply":"2021-06-19T09:24:19.937852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(traindf.columns) -1\nprint(numClasses)\ncolumns = list(traindf.columns)\ncolumns.pop()\ncolumns","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:19.940601Z","iopub.execute_input":"2021-06-19T09:24:19.941054Z","iopub.status.idle":"2021-06-19T09:24:19.952564Z","shell.execute_reply.started":"2021-06-19T09:24:19.941018Z","shell.execute_reply":"2021-06-19T09:24:19.951541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ntraindf[columns] =scaler.fit_transform(traindf[columns])\ntraindf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:19.954257Z","iopub.execute_input":"2021-06-19T09:24:19.954638Z","iopub.status.idle":"2021-06-19T09:24:25.084735Z","shell.execute_reply.started":"2021-06-19T09:24:19.954602Z","shell.execute_reply":"2021-06-19T09:24:25.083795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLoader = torch.utils.data.DataLoader(dataset=torch.tensor(traindf.values), batch_size=batchSize, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:25.086155Z","iopub.execute_input":"2021-06-19T09:24:25.086539Z","iopub.status.idle":"2021-06-19T09:24:25.285448Z","shell.execute_reply.started":"2021-06-19T09:24:25.086499Z","shell.execute_reply":"2021-06-19T09:24:25.28446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learningRate=0.03","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:25.288352Z","iopub.execute_input":"2021-06-19T09:24:25.288761Z","iopub.status.idle":"2021-06-19T09:24:25.292885Z","shell.execute_reply.started":"2021-06-19T09:24:25.28872Z","shell.execute_reply":"2021-06-19T09:24:25.291822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(nn.Linear(inputSize,numClasses),\n                      nn.ReLU(),\n                      nn.LogSoftmax(dim=1))\n              \ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learningRate)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:25.294754Z","iopub.execute_input":"2021-06-19T09:24:25.295154Z","iopub.status.idle":"2021-06-19T09:24:25.306007Z","shell.execute_reply.started":"2021-06-19T09:24:25.295116Z","shell.execute_reply":"2021-06-19T09:24:25.305069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch=50\nfor e in range(epoch):\n  running_loss=0\n  for i,data in enumerate(trainLoader):\n    labels=data[:,-1]\n    attributes=data[:,0:len(traindf.columns) -1].float()\n    logits=model(attributes)\n    loss= criterion(logits,labels.long())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    running_loss += loss.item()\n  else:\n    print(f\"Training loss: {running_loss/len(trainLoader)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:25.309479Z","iopub.execute_input":"2021-06-19T09:24:25.309784Z","iopub.status.idle":"2021-06-19T09:25:24.229467Z","shell.execute_reply.started":"2021-06-19T09:24:25.309756Z","shell.execute_reply":"2021-06-19T09:25:24.228593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testdf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:24.230708Z","iopub.execute_input":"2021-06-19T09:25:24.231204Z","iopub.status.idle":"2021-06-19T09:25:24.24919Z","shell.execute_reply.started":"2021-06-19T09:25:24.231163Z","shell.execute_reply":"2021-06-19T09:25:24.248269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for i,data in enumerate(trainLoader):\n        labels=data[:,-1]\n        attributes=data[:,0:len(traindf.columns) -1].float()\n        outputs = model(attributes)\n        # max returns (value ,index)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += labels.size(0)\n        n_correct += (predicted == labels).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\n    print(f'Accuracy of the network: {acc} %')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:24.250421Z","iopub.execute_input":"2021-06-19T09:25:24.251005Z","iopub.status.idle":"2021-06-19T09:25:25.411276Z","shell.execute_reply.started":"2021-06-19T09:25:24.250863Z","shell.execute_reply":"2021-06-19T09:25:25.410445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample=pd.read_csv('/kaggle/input/tabular-playground-series-jun-2021/sample_submission.csv')\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:25.412465Z","iopub.execute_input":"2021-06-19T09:25:25.412954Z","iopub.status.idle":"2021-06-19T09:25:25.522584Z","shell.execute_reply.started":"2021-06-19T09:25:25.412915Z","shell.execute_reply":"2021-06-19T09:25:25.521604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sample.head())\nprint(sample.shape)\nprint(testdf.shape)\nprint(traindf.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:25.524081Z","iopub.execute_input":"2021-06-19T09:25:25.524447Z","iopub.status.idle":"2021-06-19T09:25:25.534431Z","shell.execute_reply.started":"2021-06-19T09:25:25.52441Z","shell.execute_reply":"2021-06-19T09:25:25.533458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler2=StandardScaler()\ntestdf[columns] =scaler.fit_transform(testdf[columns])\ntestdf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:25.535902Z","iopub.execute_input":"2021-06-19T09:25:25.536419Z","iopub.status.idle":"2021-06-19T09:25:27.916834Z","shell.execute_reply.started":"2021-06-19T09:25:25.53638Z","shell.execute_reply":"2021-06-19T09:25:27.915252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testLoader = torch.utils.data.DataLoader(dataset=torch.tensor(testdf.values), batch_size=batchSize, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:27.918224Z","iopub.execute_input":"2021-06-19T09:25:27.918601Z","iopub.status.idle":"2021-06-19T09:25:27.998875Z","shell.execute_reply.started":"2021-06-19T09:25:27.918564Z","shell.execute_reply":"2021-06-19T09:25:27.997915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    a=torch.empty(0,9)\n    for i,data in enumerate(testLoader):\n        attributes=data.float()\n        logps = model(attributes)\n        ps = torch.exp(logps)\n        ps.shape\n        a=torch.vstack((a,ps))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:28.000165Z","iopub.execute_input":"2021-06-19T09:25:28.000609Z","iopub.status.idle":"2021-06-19T09:25:28.356363Z","shell.execute_reply.started":"2021-06-19T09:25:28.000567Z","shell.execute_reply":"2021-06-19T09:25:28.355524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(a.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:28.357579Z","iopub.execute_input":"2021-06-19T09:25:28.357929Z","iopub.status.idle":"2021-06-19T09:25:28.362621Z","shell.execute_reply.started":"2021-06-19T09:25:28.357895Z","shell.execute_reply":"2021-06-19T09:25:28.361697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result= a.numpy()\nsample.iloc[:,1:10]=result\nsample.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:28.3639Z","iopub.execute_input":"2021-06-19T09:25:28.364265Z","iopub.status.idle":"2021-06-19T09:25:28.606897Z","shell.execute_reply.started":"2021-06-19T09:25:28.364231Z","shell.execute_reply":"2021-06-19T09:25:28.605962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv('/kaggle/working/output.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:25:28.608187Z","iopub.execute_input":"2021-06-19T09:25:28.608552Z","iopub.status.idle":"2021-06-19T09:25:30.501063Z","shell.execute_reply.started":"2021-06-19T09:25:28.608515Z","shell.execute_reply":"2021-06-19T09:25:30.500225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}